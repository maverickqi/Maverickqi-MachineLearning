{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一：特征抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、字典特征抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 当数据集中有很多类别特征时：1、将数据集的特征->字典类型；2、DicrVectorizer转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# 字典特征提取\n",
    "# sklearn.feature_extraction.DictVectorizer(sparse=True)\n",
    "\n",
    "# 返回sparse矩阵\n",
    "# DictVectorizer.fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   1.   0. 100.]\n",
      " [  1.   0.   0.  60.]\n",
      " [  0.   0.   1.  30.]]\n"
     ]
    }
   ],
   "source": [
    "data = [{'city':'北京','temperature':100},{'city':'上海','temperature':60},{'city':'深圳','temperature':30}]\n",
    "\n",
    "# 实例化一个转化器类型\n",
    "transfer = DictVectorizer(sparse=False)\n",
    "\n",
    "# 调用fit_transform()\n",
    "new_data = transfer.fit_transform(data)\n",
    "\n",
    "# 特征名字\n",
    "transfer.get_feature_names()\n",
    "\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、文本特征抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "\n",
    "# 返回词频矩阵\n",
    "# sklaern.feature_extracion.text.CountVectorizer(stop_words=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 2 0 1 1]\n",
      " [1 1 0 0 1 1 0]]\n",
      "['dislike', 'lief', 'life', 'like', 'long', 'python', 'short']\n"
     ]
    }
   ],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # 英文文本 # # # # # # # #  # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "data = [\"life is short, I like like python\",\"lief is too long, I dislike python\"]\n",
    "\n",
    "# 实例化一个转化器类型\n",
    "transfer = CountVectorizer(stop_words=['is','too'])\n",
    "# 调用fit_transform()\n",
    "new_data = transfer.fit_transform(data)\n",
    "\n",
    "# 稀疏矩阵\n",
    "# print(new_data)\n",
    "# 二维矩阵\n",
    "print(new_data.toarray())\n",
    "print(transfer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0]\n",
      " [0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # 中文文本 # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "\n",
    "data = [\"我 爱 北京 天安门\",\"天安门 上 太阳 升\"]\n",
    "\n",
    "# 实例化一个转化器类型\n",
    "transfer = CountVectorizer()\n",
    "# 调用fit_transform()\n",
    "new_data = transfer.fit_transform(data)\n",
    "\n",
    "# 稀疏矩阵\n",
    "#print(new_data)\n",
    "# 二维矩阵\n",
    "print(new_data.toarray())\n",
    "#print(transfer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # #中文文本/自动分词 # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "data = [\"8月16日上午，大连市召开疫情防控新闻发布会，通报疫情最新进展和防控措施等情况。\",\n",
    "       \"大连市工业和信息化局副局长张立志介绍，大连市复工复产组工信局、商务局、要求，结合当前疫情防控形势，提出了有序恢复营业的建议。\",\n",
    "       \"考虑到部分营业机构服务对象和服务形式的特殊性，将根据疫情发展或上级主管部门要求，由行业主管部门适时决定开放时间。\"]\n",
    "\n",
    "# 分词函数\n",
    "def cut_word(text):\n",
    "    return \" \".join(list(jieba.cut(text)))\n",
    "\n",
    "data_new = [cut_word(data) for data in data]\n",
    "\n",
    "# 实例化一个转化器类型\n",
    "transfer = CountVectorizer(stop_words=[\"嘤嘤嘤\"])\n",
    "# 调用fit_transform()\n",
    "new_data = transfer.fit_transform(data_new)\n",
    "\n",
    "# 稀疏矩阵\n",
    "# print(new_data)\n",
    "# 二维矩阵\n",
    "print(new_data.toarray())\n",
    "print(transfer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、特征词抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['16', '上午', '上级', '主管部门', '介绍', '信局', '信息化', '决定', '副局长', '发展', '发布会', '召开', '商务局', '复产', '复工', '大连市', '对象', '工业', '建议', '开放', '当前', '形势', '形式', '恢复', '情况', '措施', '提出', '新闻', '时间', '最新进展', '有序', '服务', '机构', '根据', '特殊性', '疫情', '立志', '组工', '结合', '考虑', '营业', '行业', '要求', '适时', '通报', '部分', '防控']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba\n",
    "\n",
    "data = [\"8月16日上午，大连市召开疫情防控新闻发布会，通报疫情最新进展和防控措施等情况。\",\n",
    "       \"大连市工业和信息化局副局长张立志介绍，大连市复工复产组工信局、商务局、要求，结合当前疫情防控形势，提出了有序恢复营业的建议。\",\n",
    "       \"考虑到部分营业机构服务对象和服务形式的特殊性，将根据疫情发展或上级主管部门要求，由行业主管部门适时决定开放时间。\"]\n",
    "\n",
    "# 分词函数\n",
    "def cut_word(text):\n",
    "    return \" \".join(list(jieba.cut(text)))\n",
    "\n",
    "data_new = [cut_word(data) for data in data]\n",
    "\n",
    "# 实例化一个转化器类型\n",
    "transfer = TfidfVectorizer()\n",
    "# 调用fit_transform()\n",
    "new_data = transfer.fit_transform(data_new)\n",
    "\n",
    "# 稀疏矩阵\n",
    "# print(new_data)\n",
    "# 二维矩阵\n",
    "print(new_data.toarray())\n",
    "print(transfer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二：特征预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X' = (X-Xmin)/(Xmax-Xmin)         Xmax～Xmin当前列的最大值与最小值\n",
    "# X\" = X'*(mx-mi)+mi                mx~mi 为想要放缩的区间，比如0～1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "path = '/Users/zhangqihao/Desktop/PYTHON/机器学习/机器学习资料1/02-代码/dating.txt'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# 需要每一行，前3列\n",
    "data = data.iloc[:,:3]\n",
    "\n",
    "# 实例化一个转化器类型\n",
    "transfer = MinMaxScaler(feature_range=[0,1])\n",
    "# 调用fit_transform()\n",
    "new_data = transfer.fit_transform(data)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X' = (X-mean)/std                  std:标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 实例化一个转化器类型\n",
    "transfer = StandardScaler()\n",
    "# 调用fit_transform()\n",
    "new_data = transfer.fit_transform(data)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三：特征降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降维： 降低特征的个数。 效果： 特征与特征之间不相关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据中包含冗余或相关变量，从原有特征中找出主要特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### （1）Fliter过滤式： 主要探究特征本身特点、特征与特征和目标值之间关联"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方差选择法：低方差特征过滤（删除低方差的一些特征）\n",
    "\n",
    "# 特征方差小：某个特征 大多样本的值比较相近\n",
    "# 特征方差大：某个特征 很多样本的值都有差别\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2318, 7)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/zhangqihao/Desktop/PYTHON/机器学习/机器学习资料1/02-代码/factor_returns.csv'\n",
    "data = pd.read_csv(path)\n",
    "data = data.iloc[:,1:-2]\n",
    "\n",
    "# 实例化一个转化器类型\n",
    "transfer = VarianceThreshold(threshold =10)\n",
    "# 调用fit_transform()\n",
    "new_data = transfer.fit_transform(data)\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关系数: Pearson Correlation Coeifficent\n",
    "\n",
    "# -1 < r < 1\n",
    "# r > 0 为正相关；r < 0为负相关\n",
    "# |r| < 0.4为低度相关; 0.4 < |r| < 0.7为显著性相关; 0.7 < |r| < 0.4为高线性相关\n",
    "\n",
    "# 若相关性很高： 1、选取其中一个； 2、加权求和； 3、主成分分析\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.004389322779936262, 0.8327205496564927)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(data[\"pe_ratio\"], data[\"pb_ratio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### （2）Embedded嵌入式：算法自动选择特征（特征与目标值之间的关联）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树：信息熵，信息增益\n",
    "# 正则化： L1, L2\n",
    "# 深度学习：卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、主成分分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components: 小数 表示保留百分之多少的信息\n",
    "#               整数 减少到多少特征\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.2423715 , -0.04779136],\n",
       "       [-2.02483453,  1.60209606],\n",
       "       [-2.21753697, -1.5543047 ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[2,8,4,5],[3,5,8,9],[5,7,9,8]]\n",
    "\n",
    "# 实例化一个转化器类型\n",
    "transfer = PCA(n_components=2)\n",
    "# 调用fit_transform()\n",
    "new_data = transfer.fit_transform(data)\n",
    "new_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
